\section{Introduction \label{sec:intro}}
Large-scale data-processing frameworks, such as MapReduce~\cite{MapReduce}, SCOPE~\cite{SCOPE}, Hadoop~\cite{Dittrich:2010}, Spark~\cite{zaharia2010spark}, have become an integral part of computing today. One reason for their immense popularity is that they provide a simplified programming model that greatly simplifies the distribution and fault-tolerance of big-data processing. For instance, frameworks like SCOPE and Spark provide a SQL-like declarative interface for specifying the relational skeleton of data-processing jobs while providing extensibility by supporting expressions and functions written in general-purpose languages like C\#, Java, or Scala. 

%\emph{MapReduce} framework has become an immensely popular for easy development of scalable parallel applications that process large amounts of data. 
%The advantage of MapReduce is that it isolates the details of running a big data application as a distributed program. 
%To ease the use of MapReduce, several projects (Apache Pig, Apache Hive or SCOPE) provide high-level declarative interfaces on top of the MapReduce framework. 
%This means that big data processing jobs are composed of queries expressed in an SQL-like declarative language with expressions written in languages like Java, C\# or Scala.
The relational aspect is crucial: it is what enables the automatic parallelization for efficiently scaling out to arbitrary amounts of data.
Big data systems assume that the non-relational part is written carefully enough so that it does not violate the assumptions needed for automatic parallelization: e.g., programmers must write their non-relational logic to be deterministic and insensitive to the ordering of the input.

However, these systems are known to lag far behind traditional database systems in runtime efficiency~\cite{Jahani:2011,Pavlo:2009}, primarily because of the flexibility of the programming model they support. For instance,  a key bottleneck in Spark is neither the disk nor the network, but the time spent by the CPU on compression/decompression of data, serialization/deserialization of the input into/from Java objects, and the JVM garbage collection~\cite{ousterhout-nsdi15}. SCOPE,  described more fully in Section~\ref{sec:Scope}, supports a hybrid native (C++) and C\# runtime partly to alleviate this overhead.
Like SCOPE, Hadoop Streaming lets programmers write programs in a mix of languages\cite{hadoop_stream}. 
Our analysis shows that this cross-language interaction (in SCOPE, between the native and C\# runtimes) is a significant cost in the overall system.  
Equally importantly, the presence of non-relational code blocks the powerful relational optimizations implemented in these data-processing runtimes, e.g. ~\cite{guo2012spotting}. 


The goal of this work is to study and better understand the key performance bottlenecks in modern data-processing systems, and demonstrate the potential for cross-language optimizations. While this paper is primarily about SCOPE, we believe our results and optimizations generalize to other data-processing systems. 
%SCOPE (Structured Computations for Parallel Execution) is a query language that combines relational logic written in SQL with user expressions written in C\#.
SCOPE is the key data-processing system used at Microsoft running at least half a million jobs daily on serveral Microsoft data centers. 
Figure~\ref{fig:example} shows a simple example of a SCOPE program (hereafter referred to as a {\em script}) that interleaves relational logic with C\# expressions.

%To achieve this goal, we empiricaly analyse over 3,000,000 SCOPE jobs that run across five data centers at Microsoft during one week. 

%The language supports MapReduce programming model and is developed at Microsoft for big data processing. 
%Nowadays, it is used for almost 450,000 daily jobs running on several data centers in Microsoft.


%One source of inefficiency is due to non-relational code found in big data queries. 
%Despite powerfulness of SQL optimizations, such optimizations can not be applied to arbitrary code written in an imperative language.
%This calls for cross language optimizations in order to obtain a high-level efficiency of processing big data jobs.
%Moreover, we believe that increasing hardware and computing power is no longer a sustainable way of improving the efficiency of big data processing . 

%The goal of this work is to understand how the time is spent in big-data jobs and where is a potential for cross language optimizations.

%As described later in Section~\ref{sec:background}, the stage of SCOPE job can run as native (C++) if it does not contain any non-relational code. 
%However, this is unlikely the case and to run C\# code in SCOPE script the runtime performs data serialization from C++ format to format required by .NET runtime. 
%This process of data serialization and deserialization is inherently very expensive and to increase the number of job stages that run as native, the SCOPE compiler provides C++ implementation for a subset of .NET framework methods. 
%To illustrate when SCOPE script runs as native vs. non-native, consider examples in Figure~\ref{fig:example}. 
%Both code fragments are very simple SCOPE jobs. 
%The script in Figure~\ref{fig:native} runs as native, because SCOPE compiler has C++ implementation for \emph{String.isNullOrEmpty} method. On the other hand, the script in Figure~\ref{fig:non-native} has a call to \emph{Split} method, which is not optimized by the compiler and this causes entire script to run as non-native (C\# code).

\begin{figure}[ht]
 \begin{minipage}[b]{\linewidth}
  
   \begin{verbatim}
data = SELECT *
  FROM inputStream
  WHERE !String.IsNullOrEmpty(A) AND B == "Key1";
\end{verbatim}

    \subcaption{Predicate visible to optimizer.}
    \label{fig:native}
  \end{minipage}
  %
  \begin{minipage}[b]{\linewidth}
   \begin{verbatim}
data = SELECT *
  FROM inputStream
  WHERE M(A, B);

#CS
bool M(string x, string y) {
   return !String.IsNullOrEmpty(x) && y == "Key1";
}
#ENDCS
    \end{verbatim}

    \subcaption{Predicate invisible to optimizer.}
    \label{fig:non-native}
  \end{minipage}


\caption{Script Examples}
\label{fig:example}
\end{figure}

In Figure~\ref{fig:native}, the predicate in the {\tt WHERE} clause is subject to two potential optimizations:
\begin{enumerate}
\item The optimizer may choose to {\em promote} one (or both) of the conjuncts to an earlier part of the program, especially if either {\tt A} or {\tt B} are columns used for partitioning the data.
This can dramatically reduce the amount of data needed to be transferred from one vertex to another.
\item The SCOPE compiler has a set of methods that it considers to be {\em intrinsics}.
An intrinsic is a .NET method for which the SCOPE runtime has a semantically equivalent native function.
For instance, the method {\tt String.isNullOrEmpty} checks whether its argument is either null or else the empty string.
The corresponding native method is able to execute on the native data encoding which does not involve creating any .NET objects or instantiating the {\em CLR}, i.e., the .NET virtual machine.
\end{enumerate}

On the other hand, Figure~\ref{fig:non-native} shows a slight variation where the user implemented the predicate in a separate C\# method. Unfortunately, the SCOPE compiler treats the call to user-defined functions as a black box. As a result, both optimizations are disabled and the predicate is executed in a C\# virtual machine. The resulting serialization and data-copying costs can reduce the throughput of the job by as much as XXX percent. 


%has a call to \emph{Split} method, which the SCOPE compiler does not understand. This causes the ent


%which is not optimized by the compiler and this causes entire script to run as non-native (C\# code).

When facing a performance regression, a performance analyst first measures her system to understand where the bottlenecks are.  Then, she can act on those data to make her system faster.  While such an approach is well understood and easy to execute for desktop software, it becomes challenging in the context of a massive distributed system, like SCOPE.
This paper first describes \emph{how} we built a datacenter-wide profiler so we could even start to measure application bottlenecks.
Our new profiling infrastructure is a combination of offline static analysis of the executed code in addition to low-overhead online measurements captured by SCOPE's runtime.
Then, the paper describes the results of our profiling of over 3 million SCOPE programs across five data centers within Microsoft.
We find programs with non-relational code take between 45-70\% of data center CPU time.  

% %Despite knowing the cause of inefficiencies, still the little is known about how much time SCOPE jobs spend in non-relational code and what are the opportunities to trigger more generation of native code. 
% To answer these questions, we propose a new profiling infrastructure based on static analysis of job artifacts.
% By doing this, we are able to post-analyse large amount of jobs without introducing any additional overhead.
% Except the time spent in non-relational code, our profiling approach also surveys different sources of non-relational code in SCOPE jobs.
% It returns a list of most commonly used framework methods, for which having C++ implementation would trigger more generation of native code.
Finally, we discuss the effects of cross-language optimization based on \emph{method inlining}. 
By inlining a method call, the compiler/optimizer is now aware of the logic contained in the body of the method. 
We discuss the effectiveness of such optimizations in 5 case studies by optimizing jobs from 5 different teams at Microsoft. 

% Our experimental evaluation shows that non-native code takes between 45-75\% of data center CPU time. By further increasing the list of framework functions that have native implementation we can optimize up to Z\% of data center time.
% Finally, we illustrate that performance impact of inlining jobs to run as native is statistically significant and range between A and B\%. 

% As a consequence, our findings motivate future research in at least two ways.
% First, on tools and techniques that help developers write more efficient big data jobs by avoiding unnecessary generation of non-native code. 
% Second, on compiler optimizations that automatically generate native code from the non-relational logic.


% In summary, this work contributes the following:
% \begin{itemize}
% \item \emph{Profiling infrastructure for analyzing SCOPE jobs.} 
% We present the approach for profiling data centers based on static analysis of job artifacts. 
% The approach reports time spent in relational vs non-relational code and detects different sources of non-relational code for every jobs stage. 

% \item \emph{Reporting opporunities for cross-language optimizations.} We discuss two possible ways to enable further generation of native code. 
% First, we survey the most relevant framework functions relevant for native implementation. 
% Second, we present an analysis to find opportunities that trigger the generation of native code by \emph{inlining} user-written functions.  

% \item \emph{Empirical evidence.} 
% We illustrate by X case studies that enabling job stage to run as native code signifcantly improve the job performance by factors Y-K.
% \end{itemize}


