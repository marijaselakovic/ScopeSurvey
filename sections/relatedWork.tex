\section{Related Work}

\subsection{MapReduce Optimizations}
Since MapReduce~\cite{Dean:2008} is the de facto programming model for big data analytics~\cite{Gates:2009}, the performance of MapReduce is crucial to ensure optimal resource utilization and efficiency.
Various approaches have been proposed for the optimization of MapReduce jobs.
They include techniques to improve the scheduling of task execution~\cite{Isard:2009,Zaharia:2008}, to efficiently perform joins and indexing~\cite{Dittrich:2010,Floratou:2011}, an analysis for automatic work-sharing across multiple jobs~\cite{Nykiel:2010} and an extension to MapReduce model for efficiently merging the data computed by map and reduce modules~\cite{Yang:2007} . Although some of the standard database optimizations, such as filter pushdown are implemented in Pig~\cite{Olston:2008}, the recent work by Jahani et al.~\cite{Jahani:2011} suggests that many traditional query optimizations are not applicable for MapReduce because of the user-written operators. It further proposes several optimizations of \emph{map()} functions by targeting data-centric programming idioms~\cite{Jahani:2011}. Our work shows that time spent in non-relational code takes a large fraction of data center time and cross-language optimizations demonstrate a potential to significantly improve the performance of MapReduce jobs.

\subsection{Profiling MapReduce Jobs}
Many MapReduce systems, such as Hadoop~\cite{hadoop_stream}, provide facilities for monitoring cluster performance.
However, the collected metrics represent cluster-level information from which the regular user does not benefit.
Enabling users to optimize their jobs by tuning job parameters, Herodotou et al.~\cite{Herodotou} propose a dynamic binary instrumentation of the MapReduce framework to capture dataflows and costs during job execution at the task level or the phase level.
In contrast to the dynamic analysis, our approach to profiling big data jobs is purely static and based on the analysis of job artifacts. Doing this allows us to analyze a large number of jobs without introducing any additional overhead. 

\subsection{Query Optimizations}
There has been extensive work in optimizing relational queries since the early '70s~\cite{Chaudhuri:1998}. A large class of optimizations include exploiting commutativity among operators~\cite{Chaudhuri:1994,Yan:1995},reducing multi-block queries to single block~\cite{Kim:1982,Muralikrishna:1992}, using semijoin techniques to optimize multi-block queries~\cite{Mumick:1994,Seshadri:1996}, query materialization~\cite{Chaudhuri:1995,Phan:2008}, and query indexing~\cite{SELLIS1988175,Bertino:1989,Fang:2008}. Several approaches address optimizations of stored procedures (also called user-defined predicates) in relational systems~\cite{Hellerstein:1993,Chimenti:1989}. While these aforementioned techniques are powerful in optimizing relational queries, traditional query optimizers treat non-relational code as a black-box.


