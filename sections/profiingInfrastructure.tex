\section{Profiling Infrastructure for Data Centers}
Jobs run on a distributed computing platform, called Cosmos, designed for storing and analyzing massive data sets. Cosmos runs on five clusters consisting of thousands of commodity servers~\cite{SCOPE}. Cosmos is highly scalable and performant: it stores exabytes of data across hundreds of thousands of physical machines. Cosmos runs millions of big-data jobs every week and almost half million jobs every day. It is used by more than 10,000 developers at Microsoft running very diverse workloads and scenarios.

Finding optimization opportunities that are applicable to such a large number of diverse jobs is a challenging problem.
We can hope to find interesting conclusions only if our profiling infrastructure is scalable. To achieve this, the important aspect to consider is \emph{what type of information we should analyze}. In the following sections, we describe our major decisions when building infrastructure for profiling big data jobs.

\subsection{Job Artifacts}
\label{sec:artifacts}

After execution of a SCOPE job, the runtime produces several artifacts that contain code and runtime information for every job stage.
Job artifacts are indefinitely stored within Cosmos itself in a {\em job repository}.
This provides two benefits: we can derive data for a relatively large number of jobs since we do not require re-running them, and we can also answer more complex, but interesting questions, such as which job stages run as C++ vs. C\#.
We provide an overview of the subset of artifacts which we use to profile the data center.

\paragraph{Job Algebra}

The job algebra is a graph representation of the job execution plan. Job vertices are presented as outer-most nodes in a graph. Each job vertex contains all operators that run inside that vertex and an operator can be either user-defined or native. Optionally, if all operators are native, the vertex can be marked with the \emph{nativeOnly} flag, indicating that the entire vertex runs as native (C++). However, it does not distinguish between native and user-defined operators.


\paragraph{Runtime Statistics}
The runtime statistics file provides information on execution time for every job vertex and every operator inside the vertex. Among other statistics it includes CPU times, which we use as the primary metric of performance. Big data systems process significant amounts of data but often are CPU-bound\cite{ousterhout-nsdi15} due to the large overheads behind serialization and de-serialization so we measure (in addition to bytes read and written) CPU time.

%\paragraph{Binaries}
%The job repository stores a job and all third-party projects in binary format (dll) that contains both native and managed code (.NET) with the generated assembly code.


\paragraph{Generated C\# and C++ Code}
The SCOPE compiler generates both C\# and C++ code for every job.
An artifact containing the C++ code has for every vertex a code region containing a C++ implementation of the vertex and another code region that provides class names for every operator that runs as C\#.
An artifact containing the C\# code includes implementations of non-native operators and user-written classes and functions defined inside the script.
Both source and binary are available for the generated code.

\subsection{Static Analysis}

After collecting the artifacts described in Section~\ref{sec:artifacts}, we perform static analysis to detect different sources of C\# code in every vertex of a job. This is important to understand the opportunities for optimizing job vertices through C\# to C++ translation. For instance, an operator can run as managed code due to only a single method call, or because of more complex C\# code.

Figure~\ref{fig:analysis} gives an overview of our analysis. It has two main components: \emph{Analysis of C++ code} and \emph{Analysis of C\# code}. Each analysis performs at the granularity of a job vertex. The goal is to look for opportunities to run an entire vertex as C++ code, which would remove all steps of data serialization between user and native operators within the vertex.


\begin{figure}[ht]
\begin{tikzpicture}[>=latex,xscale=2.7,yscale=.85,text centered,line 
  width=.8pt]
\tikzstyle{every node}=[draw,rounded corners,drop 
shadow,minimum height=1em]
\draw (1.1, 8) node[draw,text width=2em, fill = apricot] (vi) {$V_{i}$};
\draw (0.5,8) node[draw,text width=2em, fill = apricot] (vone) {$V_{1}$};
\draw (1.7,8) node[draw,text width=2em, fill = apricot] (vn) {$V_{n}$};
\draw (1.1,7) node[draw,text width=12em, fill = myBlue] (cplus) {Analysis of C++ code};
\draw (1.1,5) node[draw,text width=12em, fill = myBlue] (cs) {Analysis of C\# code};





\tikzstyle{every node}=[]
\draw (1.1,9) node[] (alg) {Job Algebra};
\draw (2.2,7) node[] (cpluscode) {C++ code};

\draw (1.1,6) node[] (names) {Class names};
\draw (2.2,5) node[] (cscode) {C\# code};
\draw (1.1,4) node (src) {Sources of C\# code + Inlineable functions};


\draw[->] (alg) -- (vi);
\draw[->] (alg) -- (vone);
\draw[->] (alg) -- (vn);
\draw[->] (vi) -- (cplus);
\draw[->] (vone) -- (cplus);
\draw[->] (vn) -- (cplus);
\draw[->] (cplus) -- (names);
\draw[->] (names) -- (cs);
\draw[->] (cs) -- (src);
\draw[->] (cpluscode) -- (cplus);
\draw[->] (cscode) -- (cs);

\end{tikzpicture}


\caption{High level picture of static analysis}
\label{fig:analysis}
\end{figure}

The first step of the analysis is to extract the names of each job vertex, which serves as a unique identifier for the vertex. Then for each vertex, the analysis parses the generated C++ to find the class containing the vertex implementation. As discussed in Section~\ref{sec:artifacts}, for each vertex, the C++ implementation contains two code regions: one that indicates which part of a vertex runs as C++ code and another region listing class names of operators that run as C\# code. If the latter region is empty, we conclude that the entire vertex runs as C++ code. The output of the first stage of our analysis is the collection of class names that contain C\# operators.

Based on the output of the previous stage, the analysis parses the generated C\# code to find definitions and implementation of every class in the list. The implementation of managed operators contains two sources of C\# code: generated code, which we whitelist and skip in our analysis and the user-written code. After analyzing user code, the analysis outputs the following sources of method calls:
\begin{itemize}
\item .NET framework calls
\item User written functions
\item User written operators 
\end{itemize}

We are particularly interested in the first two categories. It is not unusual for an operator to execute as C\# just because of a single call to a framework method.
We want to know what are the most important framework methods to optimize to enable C++ translation for a large number of vertices. Furthermore, we observe the cases when a logic of user-written function is relatively simple, and inlining the function logic as demonstrated in Figure~\ref{fig:example} would enable generation of C++ instead of C\# code. 

The details on how we detect different sources of managed code and inlineable functions are described as follows.

\subsubsection{Detecting Sources of C\# Code}
To find .NET framework calls, it is enough to check whether a method definition comes from one of a small set of the core binaries in the .NET runtime. The analysis finds user-written functions by looking for their definition inside the script or in third-party binaries. Because the job repository keeps only binaries of third-party projects, we further analyze only user functions for which the source code is available. It is easier to optimize these functions through inlining (as described in Section~\ref{sec:analysisUser}), because we can manually confirm the correctness of inlined code. Finally, in SCOPE, users can easily implement their own operators: extractors (for parsing and constructing rows from a file), processors and reducers (for row processing), and combiners (for processing rows from two input tables). The analysis finds user operators by checking the interface the class implements. We do not consider user operators for C++ translation: they generate quite complex code which would be non-trivial to translate into C++.


\subsubsection{Analysis of User-Written Code}
\label{sec:analysisUser}
Inlining of a user-written function refers, per the standard definition, to replacing the call to the function in the script with the body of the function.
We define \emph{inlineable} methods as follows.
\begin{definition}[Inlineable method]
Method $m$ is \emph{inlineable} if it has the following properties:
\begin{itemize}
\item It contains only .NET framework calls
\item It does not contain loops and try-catch blocks
\item It does not contain any assignment statements.
\item It does not contain any references to the fields of an object.
\item For all calls inside the method, arguments are passed by value (i.e., no {\em out} parameters or call-by-reference parameters).
\end{itemize}
\end{definition}

Furthermore, we distinguish among inlineable methods those that allow for \emph{instant} C++ generation, because all called .NET framework methods are \emph{intrinsics}. However, the analysis of other inlineable functions is important, because it provides the intuition on how many vertices are potentially optimizable in this way.

%We only consider for inlineing the methods defined inside the script, because their source code is available. Even though we are aware of methods defined in third-party assemblies, because we  